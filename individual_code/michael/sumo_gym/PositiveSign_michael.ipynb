{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "import numpy as np\n",
    "\n",
    "# we need to import python modules from the $SUMO_HOME/tools directory\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "from sumolib import checkBinary \n",
    "import traci \n",
    "\n",
    "# HARDCODE\n",
    "controlled_lights = [{'name':'nodeX', 'curr_phase':0, 'num_phases': 2}]\n",
    "# uncontrolled_lights = [{'name':'nw', 'curr_phase':0, 'num_phases': 4}, {'name':'se', 'curr_phase':0, 'num_phases': 4}, {'name':'sw', 'curr_phase':0, 'num_phases': 4}]\n",
    "important_roads = ['edge1', 'edge2', 'edge3', 'edge4', 'edge5', 'edge6', 'edge7', 'edge8']\n",
    "load_options = [\"-c\", \"PositiveSign/PositiveSign.sumocfg\", \"--tripinfo-output\", \"tripinfo.xml\", '--log', 'log.txt' , \"-t\"]\n",
    "# load_options = [\"-c\", \"--tripinfo-output\", \"tripinfo.xml\", '--log', 'log.txt' , \"-t\"]\n",
    "\n",
    "class SumoEnv(gym.Env):\n",
    "    def __init__(self, steps_per_episode, render):\n",
    "        super(SumoEnv, self).__init__()\n",
    "        # self.scenario_name = scenario_name\n",
    "        self.steps_per_episode = steps_per_episode\n",
    "        self.is_done = False\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.reward_range = (-float('inf'), float('inf')) # HARDCODE\n",
    "        self.action_space = spaces.Discrete(2) # HARDCODE\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=np.array([16]), dtype=np.float32) # HARDCODE\n",
    "\n",
    "        # Start connection with sumo\n",
    "        self.noguiBinary = checkBinary('sumo')\n",
    "        self.guiBinary = checkBinary('sumo-gui')\n",
    "        # self.current_binary = self.noguiBinary\n",
    "        self.current_binary = self.guiBinary if render else self.noguiBinary\n",
    "        traci.start([self.current_binary] + load_options)\n",
    "        \n",
    "    def reset(self):\n",
    "        traci.load(load_options+[\"--start\"])\n",
    "        self.current_step = 0\n",
    "        self.is_done = False\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = []\n",
    "        wait_counts, road_counts = self._get_road_waiting_vehicle_count()\n",
    "        # HARDCODE\n",
    "        for lane in important_roads:\n",
    "            obs.append(road_counts[lane])\n",
    "            obs.append(wait_counts[lane])\n",
    "\n",
    "        return np.array(obs)\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        self._take_action(action)\n",
    "\n",
    "        traci.simulationStep()\n",
    "        self.current_step += 1\n",
    "    \n",
    "        obs = self._next_observation()\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        if self.is_done:\n",
    "            logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. \"\n",
    "                        \"You should always call 'reset()' once you receive 'done = True' \"\n",
    "                        \"-- any further steps are undefined behavior.\")\n",
    "            reward = 0.0\n",
    "\n",
    "        if self.current_step + 1 == self.steps_per_episode:\n",
    "            self.is_done = True\n",
    "\n",
    "        return obs, reward, self.is_done, {}\n",
    "\n",
    "    def _get_reward(self):\n",
    "        road_waiting_vehicles_dict , _ = self._get_road_waiting_vehicle_count()\n",
    "        reward = 0.0\n",
    "\n",
    "        for (road_id, num_vehicles) in road_waiting_vehicles_dict.items():\n",
    "            if road_id in important_roads:\n",
    "                reward -= num_vehicles\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        if action != controlled_lights[0]['curr_phase']:\n",
    "            controlled_lights[0]['curr_phase'] = action\n",
    "            self._set_tl_phase(controlled_lights[0]['name'], action)\n",
    "\n",
    "    def _get_road_waiting_vehicle_count(self):\n",
    "        wait_counts = {'edge1':0, 'edge2':0, 'edge3':0, 'edge4':0, 'edge5':0, 'edge6':0, 'edge7':0, 'edge8':0}\n",
    "        road_counts = {'edge1':0, 'edge2':0, 'edge3':0, 'edge4':0, 'edge5':0, 'edge6':0, 'edge7':0, 'edge8':0}\n",
    "        vehicles = traci.vehicle.getIDList()\n",
    "        for v in vehicles:\n",
    "            road = traci.vehicle.getRoadID(v)\n",
    "            if road in wait_counts.keys():\n",
    "                if traci.vehicle.getWaitingTime(v) > 0:\n",
    "                    wait_counts[road] += 1\n",
    "                road_counts[road] += 1\n",
    "        return wait_counts , road_counts\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        super(self)\n",
    "        traci.close()\n",
    "\n",
    "    def _set_tl_phase(self, intersection_id, phase_id):\n",
    "        traci.trafficlight.setPhase(intersection_id, phase_id)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # self.save_replay = not self.save_replay\n",
    "        self.current_binary = self.guiBinary\n",
    "        \n",
    "    def close(self):\n",
    "        self._on_training_end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the environment with an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| % time spent exploring  | 90        |\n",
      "| episodes                | 100       |\n",
      "| mean 100 episode reward | -1.15e+03 |\n",
      "| steps                   | 24651     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 80        |\n",
      "| episodes                | 200       |\n",
      "| mean 100 episode reward | -1.05e+03 |\n",
      "| steps                   | 49551     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 70       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -921     |\n",
      "| steps                   | 74451    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -697     |\n",
      "| steps                   | 99351    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -571     |\n",
      "| steps                   | 124251   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 41       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -412     |\n",
      "| steps                   | 149151   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -239     |\n",
      "| steps                   | 174051   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 22       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -142     |\n",
      "| steps                   | 198951   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 12       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -64.8    |\n",
      "| steps                   | 223851   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | -43.1    |\n",
      "| steps                   | 248751   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | -77.3    |\n",
      "| steps                   | 273651   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | -54.8    |\n",
      "| steps                   | 298551   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | -37.6    |\n",
      "| steps                   | 323451   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | -49.8    |\n",
      "| steps                   | 348351   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | -45.4    |\n",
      "| steps                   | 373251   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | -51      |\n",
      "| steps                   | 398151   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | -53.3    |\n",
      "| steps                   | 423051   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | -56.3    |\n",
      "| steps                   | 447951   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | -60.6    |\n",
      "| steps                   | 472851   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | -86.9    |\n",
      "| steps                   | 497751   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | -59.2    |\n",
      "| steps                   | 522651   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | -53.4    |\n",
      "| steps                   | 547551   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | -45.6    |\n",
      "| steps                   | 572451   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | -84.1    |\n",
      "| steps                   | 597351   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | -53.7    |\n",
      "| steps                   | 622251   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | -73      |\n",
      "| steps                   | 647151   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | -55.5    |\n",
      "| steps                   | 672051   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | -72.8    |\n",
      "| steps                   | 696951   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | -61.6    |\n",
      "| steps                   | 721851   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | -65.8    |\n",
      "| steps                   | 746751   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | -62.1    |\n",
      "| steps                   | 771651   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | -59.4    |\n",
      "| steps                   | 796551   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | -68      |\n",
      "| steps                   | 821451   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | -82.5    |\n",
      "| steps                   | 846351   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | -61.8    |\n",
      "| steps                   | 871251   |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | -58.4    |\n",
      "| steps                   | 896151   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | -61.1    |\n",
      "| steps                   | 921051   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | -103     |\n",
      "| steps                   | 945951   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | -60.1    |\n",
      "| steps                   | 970851   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | -174     |\n",
      "| steps                   | 995751   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | -98      |\n",
      "| steps                   | 1020651  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | -42.3    |\n",
      "| steps                   | 1045551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | -62.8    |\n",
      "| steps                   | 1070451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | -44.3    |\n",
      "| steps                   | 1095351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | -52.1    |\n",
      "| steps                   | 1120251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | -50.1    |\n",
      "| steps                   | 1145151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | -47.1    |\n",
      "| steps                   | 1170051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | -89.8    |\n",
      "| steps                   | 1194951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | -105     |\n",
      "| steps                   | 1219851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | -124     |\n",
      "| steps                   | 1244751  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | -109     |\n",
      "| steps                   | 1269651  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5200     |\n",
      "| mean 100 episode reward | -116     |\n",
      "| steps                   | 1294551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5300     |\n",
      "| mean 100 episode reward | -153     |\n",
      "| steps                   | 1319451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5400     |\n",
      "| mean 100 episode reward | -137     |\n",
      "| steps                   | 1344351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5500     |\n",
      "| mean 100 episode reward | -141     |\n",
      "| steps                   | 1369251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5600     |\n",
      "| mean 100 episode reward | -164     |\n",
      "| steps                   | 1394151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5700     |\n",
      "| mean 100 episode reward | -145     |\n",
      "| steps                   | 1419051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5800     |\n",
      "| mean 100 episode reward | -121     |\n",
      "| steps                   | 1443951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5900     |\n",
      "| mean 100 episode reward | -178     |\n",
      "| steps                   | 1468851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6000     |\n",
      "| mean 100 episode reward | -109     |\n",
      "| steps                   | 1493751  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6100     |\n",
      "| mean 100 episode reward | -102     |\n",
      "| steps                   | 1518651  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6200     |\n",
      "| mean 100 episode reward | -167     |\n",
      "| steps                   | 1543551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6300     |\n",
      "| mean 100 episode reward | -78.9    |\n",
      "| steps                   | 1568451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6400     |\n",
      "| mean 100 episode reward | -157     |\n",
      "| steps                   | 1593351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6500     |\n",
      "| mean 100 episode reward | -134     |\n",
      "| steps                   | 1618251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6600     |\n",
      "| mean 100 episode reward | -126     |\n",
      "| steps                   | 1643151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6700     |\n",
      "| mean 100 episode reward | -168     |\n",
      "| steps                   | 1668051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6800     |\n",
      "| mean 100 episode reward | -243     |\n",
      "| steps                   | 1692951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6900     |\n",
      "| mean 100 episode reward | -114     |\n",
      "| steps                   | 1717851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7000     |\n",
      "| mean 100 episode reward | -197     |\n",
      "| steps                   | 1742751  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7100     |\n",
      "| mean 100 episode reward | -297     |\n",
      "| steps                   | 1767651  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7200     |\n",
      "| mean 100 episode reward | -257     |\n",
      "| steps                   | 1792551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7300     |\n",
      "| mean 100 episode reward | -171     |\n",
      "| steps                   | 1817451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7400     |\n",
      "| mean 100 episode reward | -278     |\n",
      "| steps                   | 1842351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7500     |\n",
      "| mean 100 episode reward | -154     |\n",
      "| steps                   | 1867251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7600     |\n",
      "| mean 100 episode reward | -220     |\n",
      "| steps                   | 1892151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7700     |\n",
      "| mean 100 episode reward | -261     |\n",
      "| steps                   | 1917051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7800     |\n",
      "| mean 100 episode reward | -201     |\n",
      "| steps                   | 1941951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7900     |\n",
      "| mean 100 episode reward | -262     |\n",
      "| steps                   | 1966851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8000     |\n",
      "| mean 100 episode reward | -159     |\n",
      "| steps                   | 1991751  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8100     |\n",
      "| mean 100 episode reward | -197     |\n",
      "| steps                   | 2016651  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8200     |\n",
      "| mean 100 episode reward | -197     |\n",
      "| steps                   | 2041551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8300     |\n",
      "| mean 100 episode reward | -252     |\n",
      "| steps                   | 2066451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8400     |\n",
      "| mean 100 episode reward | -94.8    |\n",
      "| steps                   | 2091351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8500     |\n",
      "| mean 100 episode reward | -253     |\n",
      "| steps                   | 2116251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8600     |\n",
      "| mean 100 episode reward | -294     |\n",
      "| steps                   | 2141151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8700     |\n",
      "| mean 100 episode reward | -289     |\n",
      "| steps                   | 2166051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8800     |\n",
      "| mean 100 episode reward | -262     |\n",
      "| steps                   | 2190951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8900     |\n",
      "| mean 100 episode reward | -173     |\n",
      "| steps                   | 2215851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9000     |\n",
      "| mean 100 episode reward | -172     |\n",
      "| steps                   | 2240751  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9100     |\n",
      "| mean 100 episode reward | -292     |\n",
      "| steps                   | 2265651  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9200     |\n",
      "| mean 100 episode reward | -274     |\n",
      "| steps                   | 2290551  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9300     |\n",
      "| mean 100 episode reward | -212     |\n",
      "| steps                   | 2315451  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9400     |\n",
      "| mean 100 episode reward | -210     |\n",
      "| steps                   | 2340351  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9500     |\n",
      "| mean 100 episode reward | -288     |\n",
      "| steps                   | 2365251  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9600     |\n",
      "| mean 100 episode reward | -205     |\n",
      "| steps                   | 2390151  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9700     |\n",
      "| mean 100 episode reward | -184     |\n",
      "| steps                   | 2415051  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9800     |\n",
      "| mean 100 episode reward | -267     |\n",
      "| steps                   | 2439951  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9900     |\n",
      "| mean 100 episode reward | -129     |\n",
      "| steps                   | 2464851  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10000    |\n",
      "| mean 100 episode reward | -148     |\n",
      "| steps                   | 2489751  |\n",
      "--------------------------------------\n",
      "LEARNING TIME: 7699.726322174072\n",
      "done learning\n"
     ]
    }
   ],
   "source": [
    "# Remove TF warnings in Stable baselines (may not be safe)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import traci\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv , SubprocVecEnv\n",
    "from stable_baselines import PPO2, DQN\n",
    "\n",
    "#from env.SumoEnv import SumoEnv\n",
    "#from env.SumoEnv_Parallel import SumoEnv_Parallel\n",
    "\n",
    "import time\n",
    "\n",
    "try:\n",
    "    num_proc = 1\n",
    "\n",
    "\n",
    "    steps_per_episode = 250\n",
    "    num_episodes = 10000\n",
    "    if(num_proc == 1):\n",
    "        env = DummyVecEnv([lambda: SumoEnv(steps_per_episode, False)])\n",
    "    else:\n",
    "        env = SubprocVecEnv([lambda: SumoEnv_Parallel(steps_per_episode, False, i) for i in range(num_proc)], start_method='forkserver')\n",
    "\n",
    "    model = DQN('MlpPolicy', env, verbose=1)\n",
    "    start = time.time()\n",
    "    model.learn(total_timesteps=steps_per_episode*num_episodes)\n",
    "    print(f'LEARNING TIME: {time.time() - start}')\n",
    "    model.save('dqn_positive_10000')\n",
    "    print('done learning')\n",
    "    traci.close()\n",
    "\n",
    "except:\n",
    "    traci.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalTraCIError",
     "evalue": "connection closed by SUMO",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-58920bab68aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not connected.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_currentLabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_traceFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveStepListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistenerID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_socket\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendCmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCMD_CLOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mobjID\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"latin1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_readSubscription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"connection closed by SUMO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!BBB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: connection closed by SUMO"
     ]
    }
   ],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-env\n",
      "init\n",
      "post-env\n",
      "pre-obs\n",
      "reset\n",
      "post-obs\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TraCIException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-dfeda5c593ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f982e519f61d>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\u001b[0m in \u001b[0;36msimulationStep\u001b[1;34m(step)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0m_traceFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_currentLabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"traci.simulationStep(%s)\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"API change now handles step as floating point seconds\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendCmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubscriptionResults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"connection closed by SUMO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: connection closed by SUMO",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-dfeda5c593ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mexcept\u001b[0m \u001b[0mTraCIException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TraCIException' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv , SubprocVecEnv\n",
    "from stable_baselines import PPO2, DQN\n",
    "\n",
    "\n",
    "try:\n",
    "    steps_per_episode = 250\n",
    "\n",
    "    saved_model = \"dqn_positive_1000\"\n",
    "    print('pre-env')\n",
    "    env = DummyVecEnv([lambda: SumoEnv(steps_per_episode, True)])\n",
    "    print('post-env')\n",
    "    # wrap it\n",
    "    model = DQN.load(saved_model, env)\n",
    "    print('pre-obs')\n",
    "    obs = env.reset()\n",
    "    print('post-obs')\n",
    "    while True:\n",
    "        action, _states = model.predict(obs,deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "except TraCIException:\n",
    "    traci.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 'SUMO 1.7.0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch simulation server (SUMO-gui)\n",
    "\n",
    "current_binary = checkBinary('sumo-gui')\n",
    "load_options = [\"-c\", \"PositiveSign/PositiveSign.sumocfg\", \"--tripinfo-output\", \"tripinfo.xml\", '--log', 'log.txt' , \"-t\"]\n",
    "\n",
    "traci.start([current_binary] + load_options)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform this code while simulator server (SUMO) is open\n",
    "\n",
    "#Perform calculations here\n",
    "traci.simulationStep() # step simulation in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close() # Close TraCI connection to prevent error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_routefile(N):\n",
    "\n",
    "    with open(\"PositiveSign/PositiveSign.rou.xml\", \"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"default\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"sw-ne\" edges=\"edge1 edge4\" />\n",
    "        <route id=\"ne-sw\" edges=\"edge3 edge6\" />\"\"\", file=routes)\n",
    "        vehicle_id = 0\n",
    "        for i in range(N):\n",
    "            if i % 8 == 0:\n",
    "                print(f'    <vehicle id=\"test_{vehicle_id}\" type=\"default\" route=\"sw-ne\" depart=\"{i}\" />', file=routes)\n",
    "                vehicle_id += 1\n",
    "                print(f'    <vehicle id=\"test_{vehicle_id}\" type=\"default\" route=\"ne-sw\" depart=\"{i}\" />', file=routes)\n",
    "                vehicle_id += 1\n",
    "        print(\"</routes>\", file=routes)\n",
    "    \n",
    "generate_routefile(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_routefile(route_list, vehicle_list):\n",
    "\n",
    "    with open(\"PositiveSign/PositiveSign.rou.xml\", \"w\") as routes:\n",
    "        route_field = \"\"\"<routes>\n",
    "        <vType id=\"default\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\"\"\"\n",
    "        for route in route_list:\n",
    "            route_field += f\"\"\"<route id=\"{route['route_id']}\" edges=\"{route['edge_list']}\" />\"\"\"\n",
    "        print(route_field, file=routes)\n",
    "        \n",
    "        for vehicle in vehicle_list:\n",
    "            vehicle_id = 0\n",
    "            for i in range(vehicle['start'], vehicle['end']):\n",
    "                if i % vehicle['interval'] == 0:\n",
    "                    vehicle_field = f\"\"\"<vehicle id=\"{vehicle['batch_name']+\"_\"+str(vehicle_id)}\" type=\"default\" route=\"{vehicle['route']}\" depart=\"{i}\" />\"\"\"\n",
    "                    print(vehicle_field, file=routes)\n",
    "                    vehicle_id += 1\n",
    "        print(\"</routes>\", file=routes)\n",
    "\n",
    "route_list = [{'route_id':'A-C', 'edge_list': \"edge1 edge6\"},\n",
    "              {'route_id':'D-B', 'edge_list': \"edge7 edge4\"}]\n",
    "vehicle_list = [{'batch_name':'batch1', 'route':'A-C', 'start':0, 'end':10, 'interval':1},\n",
    "                {'batch_name':'batch2', 'route':'D-B', 'start':60, 'end':70, 'interval':1},\n",
    "                {'batch_name':'batch3', 'route':'A-C', 'start':120, 'end':130, 'interval':1},\n",
    "                {'batch_name':'batch4', 'route':'D-B', 'start':180, 'end':190, 'interval':1}\n",
    "]\n",
    "generate_routefile(route_list, vehicle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_routefile(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<route id=\"A-C\" edges=\"edge1 edge6\" />\n",
      "<route id=\"D-B\" edges=\"edge7 edge4\" />\n",
      "<vehicle id=\"batch1_0\" type=\"default\" route=\"A-C\" depart=\"0\" />\n",
      "<vehicle id=\"batch1_1\" type=\"default\" route=\"A-C\" depart=\"1\" />\n",
      "<vehicle id=\"batch1_2\" type=\"default\" route=\"A-C\" depart=\"2\" />\n",
      "<vehicle id=\"batch1_3\" type=\"default\" route=\"A-C\" depart=\"3\" />\n",
      "<vehicle id=\"batch1_4\" type=\"default\" route=\"A-C\" depart=\"4\" />\n",
      "<vehicle id=\"batch1_5\" type=\"default\" route=\"A-C\" depart=\"5\" />\n",
      "<vehicle id=\"batch1_6\" type=\"default\" route=\"A-C\" depart=\"6\" />\n",
      "<vehicle id=\"batch1_7\" type=\"default\" route=\"A-C\" depart=\"7\" />\n",
      "<vehicle id=\"batch1_8\" type=\"default\" route=\"A-C\" depart=\"8\" />\n",
      "<vehicle id=\"batch1_9\" type=\"default\" route=\"A-C\" depart=\"9\" />\n",
      "<vehicle id=\"batch1_10\" type=\"default\" route=\"A-C\" depart=\"10\" />\n",
      "<vehicle id=\"batch1_11\" type=\"default\" route=\"A-C\" depart=\"11\" />\n",
      "<vehicle id=\"batch1_12\" type=\"default\" route=\"A-C\" depart=\"12\" />\n",
      "<vehicle id=\"batch1_13\" type=\"default\" route=\"A-C\" depart=\"13\" />\n",
      "<vehicle id=\"batch1_14\" type=\"default\" route=\"A-C\" depart=\"14\" />\n",
      "<vehicle id=\"batch2_0\" type=\"default\" route=\"D-B\" depart=\"30\" />\n",
      "<vehicle id=\"batch2_1\" type=\"default\" route=\"D-B\" depart=\"31\" />\n",
      "<vehicle id=\"batch2_2\" type=\"default\" route=\"D-B\" depart=\"32\" />\n",
      "<vehicle id=\"batch2_3\" type=\"default\" route=\"D-B\" depart=\"33\" />\n",
      "<vehicle id=\"batch2_4\" type=\"default\" route=\"D-B\" depart=\"34\" />\n",
      "<vehicle id=\"batch2_5\" type=\"default\" route=\"D-B\" depart=\"35\" />\n",
      "<vehicle id=\"batch2_6\" type=\"default\" route=\"D-B\" depart=\"36\" />\n",
      "<vehicle id=\"batch2_7\" type=\"default\" route=\"D-B\" depart=\"37\" />\n",
      "<vehicle id=\"batch2_8\" type=\"default\" route=\"D-B\" depart=\"38\" />\n",
      "<vehicle id=\"batch2_9\" type=\"default\" route=\"D-B\" depart=\"39\" />\n",
      "<vehicle id=\"batch2_10\" type=\"default\" route=\"D-B\" depart=\"40\" />\n",
      "<vehicle id=\"batch2_11\" type=\"default\" route=\"D-B\" depart=\"41\" />\n",
      "<vehicle id=\"batch2_12\" type=\"default\" route=\"D-B\" depart=\"42\" />\n",
      "<vehicle id=\"batch2_13\" type=\"default\" route=\"D-B\" depart=\"43\" />\n",
      "<vehicle id=\"batch2_14\" type=\"default\" route=\"D-B\" depart=\"44\" />\n",
      "<vehicle id=\"batch3_0\" type=\"default\" route=\"A-C\" depart=\"60\" />\n",
      "<vehicle id=\"batch3_1\" type=\"default\" route=\"A-C\" depart=\"61\" />\n",
      "<vehicle id=\"batch3_2\" type=\"default\" route=\"A-C\" depart=\"62\" />\n",
      "<vehicle id=\"batch3_3\" type=\"default\" route=\"A-C\" depart=\"63\" />\n",
      "<vehicle id=\"batch3_4\" type=\"default\" route=\"A-C\" depart=\"64\" />\n",
      "<vehicle id=\"batch3_5\" type=\"default\" route=\"A-C\" depart=\"65\" />\n",
      "<vehicle id=\"batch3_6\" type=\"default\" route=\"A-C\" depart=\"66\" />\n",
      "<vehicle id=\"batch3_7\" type=\"default\" route=\"A-C\" depart=\"67\" />\n",
      "<vehicle id=\"batch3_8\" type=\"default\" route=\"A-C\" depart=\"68\" />\n",
      "<vehicle id=\"batch3_9\" type=\"default\" route=\"A-C\" depart=\"69\" />\n",
      "<vehicle id=\"batch3_10\" type=\"default\" route=\"A-C\" depart=\"70\" />\n",
      "<vehicle id=\"batch3_11\" type=\"default\" route=\"A-C\" depart=\"71\" />\n",
      "<vehicle id=\"batch3_12\" type=\"default\" route=\"A-C\" depart=\"72\" />\n",
      "<vehicle id=\"batch3_13\" type=\"default\" route=\"A-C\" depart=\"73\" />\n",
      "<vehicle id=\"batch3_14\" type=\"default\" route=\"A-C\" depart=\"74\" />\n",
      "<vehicle id=\"batch4_0\" type=\"default\" route=\"D-B\" depart=\"90\" />\n",
      "<vehicle id=\"batch4_1\" type=\"default\" route=\"D-B\" depart=\"91\" />\n",
      "<vehicle id=\"batch4_2\" type=\"default\" route=\"D-B\" depart=\"92\" />\n",
      "<vehicle id=\"batch4_3\" type=\"default\" route=\"D-B\" depart=\"93\" />\n",
      "<vehicle id=\"batch4_4\" type=\"default\" route=\"D-B\" depart=\"94\" />\n",
      "<vehicle id=\"batch4_5\" type=\"default\" route=\"D-B\" depart=\"95\" />\n",
      "<vehicle id=\"batch4_6\" type=\"default\" route=\"D-B\" depart=\"96\" />\n",
      "<vehicle id=\"batch4_7\" type=\"default\" route=\"D-B\" depart=\"97\" />\n",
      "<vehicle id=\"batch4_8\" type=\"default\" route=\"D-B\" depart=\"98\" />\n",
      "<vehicle id=\"batch4_9\" type=\"default\" route=\"D-B\" depart=\"99\" />\n",
      "<vehicle id=\"batch4_10\" type=\"default\" route=\"D-B\" depart=\"100\" />\n",
      "<vehicle id=\"batch4_11\" type=\"default\" route=\"D-B\" depart=\"101\" />\n",
      "<vehicle id=\"batch4_12\" type=\"default\" route=\"D-B\" depart=\"102\" />\n",
      "<vehicle id=\"batch4_13\" type=\"default\" route=\"D-B\" depart=\"103\" />\n",
      "<vehicle id=\"batch4_14\" type=\"default\" route=\"D-B\" depart=\"104\" />\n"
     ]
    }
   ],
   "source": [
    "routes = [{'route_id':'A-C', 'edge_list': \"edge1 edge6\"},\n",
    "          {'route_id':'D-B', 'edge_list': \"edge7 edge4\"}]\n",
    "vehicles = [{'batch_name':'batch1', 'route':'A-C', 'start':0, 'end':15, 'interval':1},\n",
    "                {'batch_name':'batch2', 'route':'D-B', 'start':30, 'end':45, 'interval':1},\n",
    "                {'batch_name':'batch3', 'route':'A-C', 'start':60, 'end':75, 'interval':1},\n",
    "                {'batch_name':'batch4', 'route':'D-B', 'start':90, 'end':105, 'interval':1}]\n",
    "\n",
    "for route in routes:\n",
    "    print(f\"\"\"<route id=\"{route['route_id']}\" edges=\"{route['edge_list']}\" />\"\"\")\n",
    "for vehicle in vehicles:\n",
    "            vehicle_id = 0\n",
    "            for i in range(vehicle['start'], vehicle['end']):\n",
    "                if i % vehicle['interval'] == 0:\n",
    "                    print(f\"\"\"<vehicle id=\"{vehicle['batch_name']+\"_\"+str(vehicle_id)}\" type=\"default\" route=\"{vehicle['route']}\" depart=\"{i}\" />\"\"\")\n",
    "                    vehicle_id += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<routes>\n",
      "        <vType id=\"default\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
      "        <route id=\"sw-ne\" edges=\"edge1 edge4\" />\n",
      "        <route id=\"ne-sw\" edges=\"edge3 edge6\" />\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"<routes>\n",
    "        <vType id=\"default\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"sw-ne\" edges=\"edge1 edge4\" />\n",
    "        <route id=\"ne-sw\" edges=\"edge3 edge6\" />\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-78-d2eb08182742>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-78-d2eb08182742>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [0:5]\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
